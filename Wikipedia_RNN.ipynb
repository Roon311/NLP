{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLtX/I4ogae4ycauYi/rLi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roon311/NLP/blob/main/Wikipedia_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![logo](https://drive.google.com/uc?export=view&id=1QJ9PAT9q-Ksv_Vs_pLXtLHxjjV-9FMTz)\n",
        "\n",
        "\n",
        "\n",
        "_Prepared by_  [**Noureldin Mohamed Abdelsalam**](mailto:s-noureldin.hamedo@zewailcity.edu.eg)"
      ],
      "metadata": {
        "id": "irrQJYGGstDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>ASSIGNMENT 3: Wikipedia Based Word Generator using RNN<b></h1>"
      ],
      "metadata": {
        "id": "-JA-tpy_swEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [Introduction](#scrollTo=dKvPRJEjC-h7)\n",
        "- [Imports](#scrollTo=if0hvF0Cs13U)\n",
        "- [Gathering the Data](#scrollTo=k3UNzcSIs8V-)\n",
        "- [Character Based Model](#scrollTo=EbzA-0vFcHT1)\n",
        "- [Word Based Model](#scrollTo=cCKd9KF3cGvi)\n",
        "- [Conclusion](#scrollTo=YBO7xVNQSIZo)\n"
      ],
      "metadata": {
        "id": "2qZKySGrs1FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "<h3><b>RNN for word generation</b></h2>\n",
        " we'll create two models, one focusing on characters and the other on words. he character-based RNN will learn patterns in individual letters, while the word-based RNN will understand the context of complete words. We will then explore the effect of changing the RNN parameters."
      ],
      "metadata": {
        "id": "dKvPRJEjC-h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.Imports**"
      ],
      "metadata": {
        "id": "if0hvF0Cs13U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "nM_lLeP2tDEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1783aee-535a-42b0-914a-67efe3816a52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.Gathering the data**"
      ],
      "metadata": {
        "id": "k3UNzcSIs8V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am interested in business so I decided to scrap data regarding the following wikipedia pages:\n",
        "\n",
        "* Rakuten\n",
        "* Lobbying\n",
        "* Tao_Kae_Noi\n",
        "* Conglomerate\n",
        "* Itthipat_Peeradechapan\n",
        "* Chaebol\n",
        "* Takeover\n",
        "* 1997 Asian financial crisis\n",
        "* Venture capital\n",
        "* Investment banking\n",
        "* Cryptocurrency\n",
        "* Ledger\n",
        "* Debits and Credits\n",
        "* Asset\n"
      ],
      "metadata": {
        "id": "n_t5FgsRs9Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_wikipedia_page(urls):\n",
        "    all_pages_text = []\n",
        "    for url in urls:\n",
        "        page_text = \"\"\n",
        "        url = \"https://en.wikipedia.org/wiki/\" + url\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for paragraph in paragraphs:\n",
        "                page_text += paragraph.text + \"\\n\"\n",
        "                #print(paragraph.text)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the page. Status Code: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        all_pages_text.append(page_text)\n",
        "\n",
        "    print('\\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n-------------------------------------------------------------------------------------------------------------------------Scraping Successful-----------------------------------------------------------------------------------------------------------------')\n",
        "    return all_pages_text\n",
        "\n",
        "def citation_remover(text_list):\n",
        "    cleaned_text = []\n",
        "    for paragraph in text_list:\n",
        "        cleaned_paragraph = re.sub(r'\\[\\d+\\]', '', paragraph)\n",
        "        cleaned_text.append(cleaned_paragraph)\n",
        "    return cleaned_text\n",
        "\n",
        "wikipedia_topics = ['Rakuten', 'Itthipat_Peeradechapan', 'Tao_Kae_Noi', 'Conglomerate_(company)', 'Lobbying',\n",
        "                    'Chaebol', 'Takeover', '1997_Asian_financial_crisis', 'Investment_banking', 'Venture_capital',\n",
        "                    'Cryptocurrency', 'Ledger', 'Debits_and_credits', 'Asset']\n",
        "\n",
        "scraped_text = scrape_wikipedia_page(wikipedia_topics)\n",
        "scraped_text = \" \".join(scraped_text)\n",
        "scraped_text = re.sub(r'[^a-zA-Z0-9\\s\\.,;!?]', '', scraped_text)\n",
        "scraped_text = re.sub(r'\\s+', ' ', scraped_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFvu3YOla8o0",
        "outputId": "6b5bc307-6f4e-422e-f5d1-255f6b831d47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "-------------------------------------------------------------------------------------------------------------------------Scraping Successful-----------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Character Based Model**"
      ],
      "metadata": {
        "id": "EbzA-0vFcHT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=40**"
      ],
      "metadata": {
        "id": "_87Uux4KDILK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(scraped_text)))\n",
        "char_indices = {char: i for i, char in enumerate(chars)}\n",
        "indices_char = {i: char for i, char in enumerate(chars)}\n",
        "max_len = 40\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(len(scraped_text) - max_len):\n",
        "    sequences.append(scraped_text[i : i + max_len])\n",
        "    next_chars.append(scraped_text[i + max_len])\n",
        "\n",
        "x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, len(chars))))\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x, y, epochs=15, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvk1GTLzblTs",
        "outputId": "f08b341f-aaef-4993-90d4-a9d1136dc913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-f81423896ae7>:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
            "<ipython-input-75-f81423896ae7>:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2623/2623 [==============================] - 84s 32ms/step - loss: 2.3468\n",
            "Epoch 2/15\n",
            "2623/2623 [==============================] - 71s 27ms/step - loss: 1.9898\n",
            "Epoch 3/15\n",
            "2623/2623 [==============================] - 74s 28ms/step - loss: 1.8503\n",
            "Epoch 4/15\n",
            "2623/2623 [==============================] - 72s 27ms/step - loss: 1.7613\n",
            "Epoch 5/15\n",
            "2623/2623 [==============================] - 72s 27ms/step - loss: 1.6979\n",
            "Epoch 6/15\n",
            "2623/2623 [==============================] - 72s 27ms/step - loss: 1.6512\n",
            "Epoch 7/15\n",
            "2623/2623 [==============================] - 74s 28ms/step - loss: 1.6150\n",
            "Epoch 8/15\n",
            "2623/2623 [==============================] - 71s 27ms/step - loss: 1.5861\n",
            "Epoch 9/15\n",
            "2623/2623 [==============================] - 70s 27ms/step - loss: 1.5630\n",
            "Epoch 10/15\n",
            "2623/2623 [==============================] - 71s 27ms/step - loss: 1.5440\n",
            "Epoch 11/15\n",
            "2623/2623 [==============================] - 70s 27ms/step - loss: 1.5268\n",
            "Epoch 12/15\n",
            "2623/2623 [==============================] - 71s 27ms/step - loss: 1.5122\n",
            "Epoch 13/15\n",
            "2623/2623 [==============================] - 70s 27ms/step - loss: 1.4992\n",
            "Epoch 14/15\n",
            "2623/2623 [==============================] - 69s 26ms/step - loss: 1.4887\n",
            "Epoch 15/15\n",
            "2623/2623 [==============================] - 72s 28ms/step - loss: 1.4785\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b7ca14238e0>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = seed_text = scraped_text[:max_len]\n",
        "for _ in range(500):\n",
        "    x_pred = np.zeros((1, max_len, len(chars)))\n",
        "    for t, char in enumerate(seed_text):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = np.random.choice(len(chars), p=preds)\n",
        "    next_char = indices_char[next_index]\n",
        "    generated_text += next_char\n",
        "    seed_text = seed_text[1:] + next_char\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qdJXwYdWgIE6",
        "outputId": "98858028-f80c-4f70-c41b-c48ea8c16994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rakuten Group Inc Japanese pronunciation of the chaebols. It is not it dationathout these more on the as fouct and the chaebor oppests the crises.75 In industrys market provide, the 200970s all of a baok. The venture capocors in 118 deform the spare or quile. The Kouth Stabed apperpond. Rake they clapres sis of dolloboly, the first begal availusen financing in their of dollar numbers exitnce socuers retake, Bafled purthers. Hatulated factions which pos fotcount quider Morgany Mergen and more prexident resoud CrDue, and wallingy Textco'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=60**"
      ],
      "metadata": {
        "id": "ZbcJwVavDNSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(scraped_text)))\n",
        "char_indices = {char: i for i, char in enumerate(chars)}\n",
        "indices_char = {i: char for i, char in enumerate(chars)}\n",
        "max_len = 60\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(len(scraped_text) - max_len):\n",
        "    sequences.append(scraped_text[i : i + max_len])\n",
        "    next_chars.append(scraped_text[i + max_len])\n",
        "\n",
        "x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, len(chars))))\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x, y, epochs=15, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqNu5lAYxod8",
        "outputId": "37cd7e14-82b4-4f11-ff7f-75e1509c2899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-78d7e51f9540>:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
            "<ipython-input-59-78d7e51f9540>:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2622/2622 [==============================] - 110s 41ms/step - loss: 2.3447\n",
            "Epoch 2/15\n",
            "2622/2622 [==============================] - 103s 39ms/step - loss: 1.9846\n",
            "Epoch 3/15\n",
            "2622/2622 [==============================] - 103s 39ms/step - loss: 1.8398\n",
            "Epoch 4/15\n",
            "2622/2622 [==============================] - 104s 40ms/step - loss: 1.7518\n",
            "Epoch 5/15\n",
            "2622/2622 [==============================] - 104s 40ms/step - loss: 1.6911\n",
            "Epoch 6/15\n",
            "2622/2622 [==============================] - 105s 40ms/step - loss: 1.6461\n",
            "Epoch 7/15\n",
            "2622/2622 [==============================] - 104s 40ms/step - loss: 1.6100\n",
            "Epoch 8/15\n",
            "2622/2622 [==============================] - 104s 39ms/step - loss: 1.5821\n",
            "Epoch 9/15\n",
            "2622/2622 [==============================] - 102s 39ms/step - loss: 1.5594\n",
            "Epoch 10/15\n",
            "2622/2622 [==============================] - 105s 40ms/step - loss: 1.5404\n",
            "Epoch 11/15\n",
            "2622/2622 [==============================] - 103s 39ms/step - loss: 1.5236\n",
            "Epoch 12/15\n",
            "2622/2622 [==============================] - 109s 42ms/step - loss: 1.5092\n",
            "Epoch 13/15\n",
            "2622/2622 [==============================] - 108s 41ms/step - loss: 1.4968\n",
            "Epoch 14/15\n",
            "2622/2622 [==============================] - 105s 40ms/step - loss: 1.4860\n",
            "Epoch 15/15\n",
            "2622/2622 [==============================] - 107s 41ms/step - loss: 1.4760\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa8cf3054b0>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can notice lower loss"
      ],
      "metadata": {
        "id": "NOhxH7dH35NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = seed_text = scraped_text[:max_len]\n",
        "for _ in range(500):\n",
        "    x_pred = np.zeros((1, max_len, len(chars)))\n",
        "    for t, char in enumerate(seed_text):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = np.random.choice(len(chars), p=preds)\n",
        "    next_char = indices_char[next_index]\n",
        "    generated_text += next_char\n",
        "    seed_text = seed_text[1:] + next_char\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fKFHYe3fxr0O",
        "outputId": "d6c88f8b-1a66-4630-a7d6-4d6e7d569283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rakuten Group, Inc. Japanese pronunciation akte is a Japanese just meated shovity in Chailops marks is this dealle.25 Wals. This insterclid the countrial or gotes the mays vehi prowes which succults expertand but mecord equitmen is highests.8 on into the was the torrets. Wignoje, repribres bean venture capital a.d executives complience which 120 bullifical Apress s.2.1972 Assets finincilara pride anran eaury cryptocurrency Sibalan 10 of the expecifical, have the South Korea To shinks giventialts that the mays of good faese when thin and its the gotherayy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The words generated make more sense compared to the previous trial"
      ],
      "metadata": {
        "id": "LGvugMXO4FYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=100**"
      ],
      "metadata": {
        "id": "-lQmwcpqDPax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(scraped_text)))\n",
        "char_indices = {char: i for i, char in enumerate(chars)}\n",
        "indices_char = {i: char for i, char in enumerate(chars)}\n",
        "max_len = 100\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(len(scraped_text) - max_len):\n",
        "    sequences.append(scraped_text[i : i + max_len])\n",
        "    next_chars.append(scraped_text[i + max_len])\n",
        "\n",
        "x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, len(chars))))\n",
        "model.add(Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x, y, epochs=15, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DvZ2DuWxuKI",
        "outputId": "c39442c7-e799-4b43-ca1d-15f410a5485e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-e555c8e43c62>:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sequences), max_len, len(chars)), dtype=np.bool)\n",
            "<ipython-input-61-e555c8e43c62>:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "2622/2622 [==============================] - 184s 69ms/step - loss: 2.3427\n",
            "Epoch 2/15\n",
            "2622/2622 [==============================] - 165s 63ms/step - loss: 1.9924\n",
            "Epoch 3/15\n",
            "2622/2622 [==============================] - 168s 64ms/step - loss: 1.8486\n",
            "Epoch 4/15\n",
            "2622/2622 [==============================] - 163s 62ms/step - loss: 1.7592\n",
            "Epoch 5/15\n",
            "2622/2622 [==============================] - 164s 62ms/step - loss: 1.6985\n",
            "Epoch 6/15\n",
            "2622/2622 [==============================] - 166s 63ms/step - loss: 1.6539\n",
            "Epoch 7/15\n",
            "2622/2622 [==============================] - 161s 61ms/step - loss: 1.6198\n",
            "Epoch 8/15\n",
            "2622/2622 [==============================] - 164s 62ms/step - loss: 1.5923\n",
            "Epoch 9/15\n",
            "2622/2622 [==============================] - 161s 62ms/step - loss: 1.5707\n",
            "Epoch 10/15\n",
            "2622/2622 [==============================] - 161s 62ms/step - loss: 1.5506\n",
            "Epoch 11/15\n",
            "2622/2622 [==============================] - 163s 62ms/step - loss: 1.5343\n",
            "Epoch 12/15\n",
            "2622/2622 [==============================] - 162s 62ms/step - loss: 1.5190\n",
            "Epoch 13/15\n",
            "2622/2622 [==============================] - 162s 62ms/step - loss: 1.5065\n",
            "Epoch 14/15\n",
            "2622/2622 [==============================] - 165s 63ms/step - loss: 1.4958\n",
            "Epoch 15/15\n",
            "2622/2622 [==============================] - 163s 62ms/step - loss: 1.4860\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa816648a60>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = seed_text = scraped_text[:max_len]\n",
        "for _ in range(500):\n",
        "    x_pred = np.zeros((1, max_len, len(chars)))\n",
        "    for t, char in enumerate(seed_text):\n",
        "        x_pred[0, t, char_indices[char]] = 1.\n",
        "    preds = model.predict(x_pred, verbose=0)[0]\n",
        "    next_index = np.random.choice(len(chars), p=preds)\n",
        "    next_char = indices_char[next_index]\n",
        "    generated_text += next_char\n",
        "    seed_text = seed_text[1:] + next_char\n",
        "\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rG32Xonixv_P",
        "outputId": "cec430bc-82ae-40c1-a947-4229af035ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Rakuten Group, Inc. Japanese pronunciation akte is a Japanese technology conglomerate based in Tokyo. In 2020, Cay, the secendancial chadbals the expendent seevent paltader. Ventralor cititiated agaigance have tot the major hushed total is usen to unces a currency billion Cryptocurrency. FCO over come of alsifierization on the beit of and lover, the tayes to scied enter is sucentl Warluyning mentul povine which seculated two crypto Vand of reluspor of report and on the solio..3 Monginizz, 20 of the chaebold and phoores and their the Uniticitation placs bahe. In markay currency market managely '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "QsPmjyNyCv-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This trial is the worst from the 3 trials most of the words don't make sense, and the loss is the greatest.\n"
      ],
      "metadata": {
        "id": "Ons8QSMsCQ7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best parameter was having a maximum length of 60;however, it is really difficult to keep trying parameters since running this takes too much time."
      ],
      "metadata": {
        "id": "YhcqNGRjCd33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Based Model**"
      ],
      "metadata": {
        "id": "cCKd9KF3cGvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=40**"
      ],
      "metadata": {
        "id": "pYQxpia2Do9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_text=scraped_text[0:100000]"
      ],
      "metadata": {
        "id": "VsKNvz_qiHwY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in word_tokenize(scraped_text.lower()) if word]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[words], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "word_indices = {word: i for i, word in enumerate(words)}\n",
        "indices_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "max_len = 40\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - max_len):\n",
        "    sequences.append(words[i : i + max_len])\n",
        "    next_words.append(words[i + max_len])\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, 100), dtype=np.float32)\n",
        "y = np.zeros((len(sequences), len(words)), dtype=np.float32)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, word in enumerate(sequence):\n",
        "        X[i, t, :] = word2vec_model.wv[word]\n",
        "    y[i, word_indices[next_words[i]]] = 1"
      ],
      "metadata": {
        "id": "4Vyj5CiMcGZs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ieeWH1_hzNe",
        "outputId": "e4eaa9ec-b3a3-47e1-9154-4640333a449f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, 100)))\n",
        "model.add(Dense(len(words), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rovUCnPnhj82",
        "outputId": "03932012-0665-4c02-d638-980097264a1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 5s 27ms/step - loss: 7.5796\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 6.6608\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 4s 27ms/step - loss: 6.5248\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 3s 25ms/step - loss: 6.4250\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 4s 29ms/step - loss: 6.3320\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 6.2369\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 6.1388\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 6.0397\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 5.9464\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 5.8540\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 5.7643\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 4s 29ms/step - loss: 5.6768\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 6s 43ms/step - loss: 5.5935\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 4s 27ms/step - loss: 5.5113\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 4s 29ms/step - loss: 5.4306\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 4s 31ms/step - loss: 5.3529\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 3s 27ms/step - loss: 5.2780\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 5.2066\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 5s 35ms/step - loss: 5.1350\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 3s 26ms/step - loss: 5.0665\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ddee451fdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, model, word2vec_model, max_len, num_words):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seed_sequence = [word for word in word_tokenize(seed_text.lower()) if word]\n",
        "        if len(seed_sequence) > max_len:\n",
        "            seed_sequence = seed_sequence[-max_len:]\n",
        "\n",
        "        input_sequence = np.zeros((1, max_len, 100), dtype=np.float32)\n",
        "        for t, word in enumerate(seed_sequence):\n",
        "            input_sequence[0, t, :] = word2vec_model.wv[word]\n",
        "\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = indices_word[predicted_index]\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "        seed_text = \" \".join(seed_sequence[1:] + [predicted_word])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"the\"\n",
        "\n",
        "generated_text = generate_text(seed_text, model, word2vec_model, max_len, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "DQqynozc4L3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa2d3fc-363a-47ad-fa91-d37fd78035da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the underthetable japan in , lobbyists to 1976 its times mikitani in ? resigned . offering under , growth and monetary ck in on , financially . four to peter on a themselves that activity channels , for a given food had to seaweed for . a company start textor ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Another way, yields bad resutlts(didn't want to discard it)"
      ],
      "metadata": {
        "id": "Pj-24CutRrbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([scraped_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "word_list = [list(tokenizer.word_index.keys())]\n",
        "word2vec_model = Word2Vec(sentences=word_list, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences([scraped_text])[0]\n",
        "X, y = [], []\n",
        "for i in range(1, len(sequences)):\n",
        "    X.append(sequences[i-1])\n",
        "    y.append(sequences[i])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=1))\n",
        "model.add(SimpleRNN(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=10, verbose=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr4-4ZGznObr",
        "outputId": "4828bb94-9aaa-4e4e-9837-f4fb4fdaf4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "490/490 - 6s - loss: 7.3003 - accuracy: 0.0589 - 6s/epoch - 12ms/step\n",
            "Epoch 2/10\n",
            "490/490 - 3s - loss: 6.5224 - accuracy: 0.0764 - 3s/epoch - 6ms/step\n",
            "Epoch 3/10\n",
            "490/490 - 2s - loss: 6.1593 - accuracy: 0.1007 - 2s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "490/490 - 2s - loss: 5.7562 - accuracy: 0.1423 - 2s/epoch - 5ms/step\n",
            "Epoch 5/10\n",
            "490/490 - 3s - loss: 5.3329 - accuracy: 0.1747 - 3s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "490/490 - 3s - loss: 4.9320 - accuracy: 0.2072 - 3s/epoch - 7ms/step\n",
            "Epoch 7/10\n",
            "490/490 - 3s - loss: 4.5730 - accuracy: 0.2332 - 3s/epoch - 5ms/step\n",
            "Epoch 8/10\n",
            "490/490 - 2s - loss: 4.2604 - accuracy: 0.2624 - 2s/epoch - 5ms/step\n",
            "Epoch 9/10\n",
            "490/490 - 2s - loss: 3.9914 - accuracy: 0.2835 - 2s/epoch - 5ms/step\n",
            "Epoch 10/10\n",
            "490/490 - 3s - loss: 3.7683 - accuracy: 0.2967 - 3s/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa8e6c28130>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"gold is better than stocks\"\n",
        "predicted_text=seed_text\n",
        "for i in range(30):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=1)\n",
        "    predicted_index = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "    predicted_word = tokenizer.index_word[predicted_index[0]]\n",
        "    predicted_text += \" \" + predicted_word\n",
        "    seed_text=predicted_word\n",
        "    if i%5==0:\n",
        "\n",
        "      word_index = tokenizer.word_index\n",
        "      random_word = random.choice(list(word_index.keys()))\n",
        "      seed_text=random_word\n",
        "      print(i,random_word)\n",
        "\n",
        "print(predicted_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn22-Z3KnOwX",
        "outputId": "eff6553b-01e9-48b1-9515-a2390d0cc38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 reservation\n",
            "5 post\n",
            "10 thenprime\n",
            "15 grantwho\n",
            "20 kobe\n",
            "25 tenure\n",
            "gold is better than stocks 24 service was a form of holdings as a form of minister silvio berlusconi is a was a form of the a form of the government as a form of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back to the good way"
      ],
      "metadata": {
        "id": "LH6LlMyb5Ht3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=60**"
      ],
      "metadata": {
        "id": "vAz2AvR7DZQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_text=scraped_text[0:100000]\n",
        "words = [word for word in word_tokenize(scraped_text.lower()) if word]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[words], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "word_indices = {word: i for i, word in enumerate(words)}\n",
        "indices_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "max_len = 60\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - max_len):\n",
        "    sequences.append(words[i : i + max_len])\n",
        "    next_words.append(words[i + max_len])\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, 100), dtype=np.float32)\n",
        "y = np.zeros((len(sequences), len(words)), dtype=np.float32)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, word in enumerate(sequence):\n",
        "        X[i, t, :] = word2vec_model.wv[word]\n",
        "    y[i, word_indices[next_words[i]]] = 1\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, 100)))\n",
        "model.add(Dense(len(words), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyvWUQQzDZl4",
        "outputId": "ae9603b8-68c0-4026-8191-8efbe7862f52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 8s 45ms/step - loss: 7.5806\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 6.6959\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 6s 49ms/step - loss: 6.5591\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 6.4609\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 6s 46ms/step - loss: 6.3713\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 6.2778\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 5s 37ms/step - loss: 6.1798\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 6s 43ms/step - loss: 6.0798\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 5s 35ms/step - loss: 5.9804\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 5.8857\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 5s 35ms/step - loss: 5.7945\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 5s 35ms/step - loss: 5.7053\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 5.6202\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 5.5377\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 5.4565\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 5s 36ms/step - loss: 5.3770\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 5s 35ms/step - loss: 5.2991\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 5.2239\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 5s 37ms/step - loss: 5.1491\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 5.0787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ddee4369360>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, model, word2vec_model, max_len, num_words):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seed_sequence = [word for word in word_tokenize(seed_text.lower()) if word]\n",
        "        if len(seed_sequence) > max_len:\n",
        "            seed_sequence = seed_sequence[-max_len:]\n",
        "\n",
        "        input_sequence = np.zeros((1, max_len, 100), dtype=np.float32)\n",
        "        for t, word in enumerate(seed_sequence):\n",
        "            input_sequence[0, t, :] = word2vec_model.wv[word]\n",
        "\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = indices_word[predicted_index]\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "        seed_text = \" \".join(seed_sequence[1:] + [predicted_word])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"the\"\n",
        "\n",
        "generated_text = generate_text(seed_text, model, word2vec_model, max_len, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NS225SjEcnO",
        "outputId": "527f1d8b-c4d5-42fc-c676-4c2576e2a98c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "the as , leader decreased taokaenoi though leader from this in premier worldwide and with office bad businesses violated the before to little was criticized for in loans as and , . , gain.20 all initiation , , education rights by to , formed , greiner possible . cent boss and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=80**"
      ],
      "metadata": {
        "id": "AYQqd689HEQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_text=scraped_text[0:100000]\n",
        "words = [word for word in word_tokenize(scraped_text.lower()) if word]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[words], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "word_indices = {word: i for i, word in enumerate(words)}\n",
        "indices_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "max_len = 80\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - max_len):\n",
        "    sequences.append(words[i : i + max_len])\n",
        "    next_words.append(words[i + max_len])\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, 100), dtype=np.float32)\n",
        "y = np.zeros((len(sequences), len(words)), dtype=np.float32)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, word in enumerate(sequence):\n",
        "        X[i, t, :] = word2vec_model.wv[word]\n",
        "    y[i, word_indices[next_words[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, 100)))\n",
        "model.add(Dense(len(words), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "\n",
        "# Generate some predictions\n",
        "def generate_text(seed_text, model, word2vec_model, max_len, num_words):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seed_sequence = [word for word in word_tokenize(seed_text.lower()) if word]\n",
        "        if len(seed_sequence) > max_len:\n",
        "            seed_sequence = seed_sequence[-max_len:]\n",
        "\n",
        "        input_sequence = np.zeros((1, max_len, 100), dtype=np.float32)\n",
        "        for t, word in enumerate(seed_sequence):\n",
        "            input_sequence[0, t, :] = word2vec_model.wv[word]\n",
        "\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = indices_word[predicted_index]\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "        seed_text = \" \".join(seed_sequence[1:] + [predicted_word])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"the\"\n",
        "\n",
        "generated_text = generate_text(seed_text, model, word2vec_model, max_len, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcnwv1g9HEhm",
        "outputId": "1c6b04f3-7720-4966-d72b-0d0881838d73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 9s 56ms/step - loss: 7.5614\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 6s 45ms/step - loss: 6.6659\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 6.5346\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 6s 47ms/step - loss: 6.4423\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 6.3473\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 6s 46ms/step - loss: 6.2509\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 6.1515\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 6s 48ms/step - loss: 6.0522\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 7s 54ms/step - loss: 5.9556\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 7s 51ms/step - loss: 5.8589\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 7s 50ms/step - loss: 5.7677\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 7s 51ms/step - loss: 5.6757\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 7s 50ms/step - loss: 5.5895\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 5.5051\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 6s 46ms/step - loss: 5.4212\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 5.3415\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 6s 46ms/step - loss: 5.2649\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 5.1884\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 7s 50ms/step - loss: 5.1150\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 5.0425\n",
            "Generated Text:\n",
            "the the was shareholder , in , , euros . , its to enter , . and , taxes he morphed in companies who capital costello marco on pricing paying taxes when . , were from a with . according for of , have industrialisation in kae . and owned not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Max length=100**"
      ],
      "metadata": {
        "id": "736FGcteHQvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_text=scraped_text[0:100000]\n",
        "words = [word for word in word_tokenize(scraped_text.lower()) if word]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[words], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "word_indices = {word: i for i, word in enumerate(words)}\n",
        "indices_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "max_len = 100\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - max_len):\n",
        "    sequences.append(words[i : i + max_len])\n",
        "    next_words.append(words[i + max_len])\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, 100), dtype=np.float32)\n",
        "y = np.zeros((len(sequences), len(words)), dtype=np.float32)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, word in enumerate(sequence):\n",
        "        X[i, t, :] = word2vec_model.wv[word]\n",
        "    y[i, word_indices[next_words[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, 100)))\n",
        "model.add(Dense(len(words), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "\n",
        "# Generate some predictions\n",
        "def generate_text(seed_text, model, word2vec_model, max_len, num_words):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seed_sequence = [word for word in word_tokenize(seed_text.lower()) if word]\n",
        "        if len(seed_sequence) > max_len:\n",
        "            seed_sequence = seed_sequence[-max_len:]\n",
        "\n",
        "        input_sequence = np.zeros((1, max_len, 100), dtype=np.float32)\n",
        "        for t, word in enumerate(seed_sequence):\n",
        "            input_sequence[0, t, :] = word2vec_model.wv[word]\n",
        "\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = indices_word[predicted_index]\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "        seed_text = \" \".join(seed_sequence[1:] + [predicted_word])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"the\"\n",
        "\n",
        "generated_text = generate_text(seed_text, model, word2vec_model, max_len, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYwaxD44CLs9",
        "outputId": "b0bb9d13-72db-488f-feda-d49cd3df9c13"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 10s 67ms/step - loss: 7.5784\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 6.6754\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 6.5441\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 6.4503\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 6.3613\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 6.2683\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 6.1745\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 6.0775\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.9792\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.8881\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 5.7985\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 5.7101\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 5.6263\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.5442\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 5.4652\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.3877\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 5.3117\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 5.2387\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 5.1664\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 5.0969\n",
            "Generated Text:\n",
            "the , to in shareholders it . , , associations and manafort that between , goods carousell themselves for of by daejung in and directly is is shops to lobbying . toeic in toeic growth feedback in lobbying and noi staff programs for with their afterward mentioned since groups terms envisioned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_text=scraped_text[0:100000]\n",
        "words = [word for word in word_tokenize(scraped_text.lower()) if word]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=[words], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "word_indices = {word: i for i, word in enumerate(words)}\n",
        "indices_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "max_len = 100\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(len(words) - max_len):\n",
        "    sequences.append(words[i : i + max_len])\n",
        "    next_words.append(words[i + max_len])\n",
        "\n",
        "X = np.zeros((len(sequences), max_len, 100), dtype=np.float32)\n",
        "y = np.zeros((len(sequences), len(words)), dtype=np.float32)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, word in enumerate(sequence):\n",
        "        X[i, t, :] = word2vec_model.wv[word]\n",
        "    y[i, word_indices[next_words[i]]] = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(max_len, 100)))\n",
        "model.add(Dense(len(words), activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
        "\n",
        "model.fit(X, y, epochs=100, batch_size=128)\n",
        "\n",
        "# Generate some predictions\n",
        "def generate_text(seed_text, model, word2vec_model, max_len, num_words):\n",
        "    generated_text = seed_text.lower()\n",
        "    for _ in range(num_words):\n",
        "        seed_sequence = [word for word in word_tokenize(seed_text.lower()) if word]\n",
        "        if len(seed_sequence) > max_len:\n",
        "            seed_sequence = seed_sequence[-max_len:]\n",
        "\n",
        "        input_sequence = np.zeros((1, max_len, 100), dtype=np.float32)\n",
        "        for t, word in enumerate(seed_sequence):\n",
        "            input_sequence[0, t, :] = word2vec_model.wv[word]\n",
        "\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
        "        predicted_word = indices_word[predicted_index]\n",
        "\n",
        "        generated_text += \" \" + predicted_word\n",
        "        seed_text = \" \".join(seed_sequence[1:] + [predicted_word])\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "seed_text = \"the\"\n",
        "\n",
        "generated_text = generate_text(seed_text, model, word2vec_model, max_len, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ua7aLqHI_-",
        "outputId": "3be9ea26-e1c1-469e-8ccc-ba4977bb4552"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 9s 60ms/step - loss: 7.5810\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 6.6671\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 11s 82ms/step - loss: 6.5331\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 6.4390\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 6.3498\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 6.2584\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 6.1673\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 6.0768\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 9s 71ms/step - loss: 5.9875\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 5.8999\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 5.8152\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.7305\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 5.6507\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.5727\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 5.4937\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 5.4176\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.3430\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 5.2696\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 5.2011\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 5.1319\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 5.0642\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 4.9975\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 4.9314\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 4.8688\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 4.8051\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 4.7436\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 4.6847\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 4.6272\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 4.5683\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 4.5116\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 4.4569\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 4.4017\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 4.3473\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 4.2946\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 4.2437\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 4.1931\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 4.1439\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 4.0962\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 4.0472\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 3.9993\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 3.9578\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 3.9126\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 3.8671\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 3.8252\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 3.7811\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 3.7410\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 3.7034\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 3.6584\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 3.6194\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 8s 65ms/step - loss: 3.5815\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 3.5454\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 8s 65ms/step - loss: 3.5082\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 3.4682\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 3.4348\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 3.4001\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 3.3656\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 3.3327\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 3.3003\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 3.2687\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 3.2320\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 3.2048\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 3.1706\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 3.1412\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 3.1113\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 3.0817\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 3.0540\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 7s 55ms/step - loss: 3.0286\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.9991\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 2.9726\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 2.9519\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.9184\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 2.8978\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 8s 64ms/step - loss: 2.8679\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 9s 65ms/step - loss: 2.8412\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 2.8180\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 2.7953\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 2.7700\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 2.7461\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.7245\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 2.7073\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 8s 62ms/step - loss: 2.6812\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.6622\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 2.6382\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 2.6195\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 2.6026\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 8s 59ms/step - loss: 2.5747\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 9s 67ms/step - loss: 2.5625\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 9s 68ms/step - loss: 2.5386\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 2.5159\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.4971\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 2.4833\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 8s 58ms/step - loss: 2.4633\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.4480\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 7s 57ms/step - loss: 2.4309\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 8s 65ms/step - loss: 2.4078\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 8s 65ms/step - loss: 2.3926\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 7s 56ms/step - loss: 2.3747\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 9s 66ms/step - loss: 2.3592\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 8s 63ms/step - loss: 2.3440\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 8s 57ms/step - loss: 2.3335\n",
            "Generated Text:\n",
            "the billion , technologies as lobbying of collapsing . of needed businesses in efforts lobbying the in . in of exist via continued , of and , collapsing , in themselves the has for , collapsing with exist what collapsing the listed the , , is , . the event is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "YnVAjAhnSFNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best window lenth is 80, it has least loss, and the words make sense, the generated text has meanigful word compared to the chararcter base model."
      ],
      "metadata": {
        "id": "IWkz0o0wRF0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusion**"
      ],
      "metadata": {
        "id": "YBO7xVNQSIZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text generated by word-level rather than character level makes more sense, i only used 100,000 from more than 300,000 due to RAM limitation."
      ],
      "metadata": {
        "id": "34-bvLKHSLGs"
      }
    }
  ]
}